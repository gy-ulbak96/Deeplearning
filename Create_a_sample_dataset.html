# Create a sample dataset

<h2> what makes a feature good?</h2>

1. Be related to the objective scale.
2. Be known at prediction time.
3. Be numeric with meaningful magnitude.
4. Have enough examples.
5. Bring human insight to problem.

<h3> Hash & Mod </h3>

trianing, validation 두 데이터 집합을 겹치지 않게 하기 위해서 반복적으로 처리

hashing : 많은 양의 data를 그보다 작은 크기의 테이블로 저장시켜 저장할 수 있게 하는 데이터 관리 기법

mod : Develop your tensorflow code on a small subset of data, then scale it out to the cloud.

---

전체 BiGQuery 데이터 셋을 샘플링하여 더 작은 데이터 셋을 생성하는 작업이다.

- Sample a BigQuery dataset to create datasets for ML
- Preprocess data using Pandas

> ##change these to try this notebook out
>
> **BUCKET = 'cloud-training-demos-ml'**
> **PROJECT = 'cloud-training-demos'**
> **REGION = 'asia-east1-a'**

> **import os**
> **os.environ['BUCKET'] = BUCKET**
> **os.environ['PROJECT'] = PROJECT**
> **os.environ['REGION'] = REGION**

> **%%bash**
> **if ! gsutil ls | grep -q gs://${BUCKET}/; then**
>   **gsutil mb -l ${REGION} gs://${BUCKET}**
> **fi**

<h2>Create ML dataset by sampling using BigQuery</h2>
Let's sample the BigQuery data to create smaller datasets.

> ##Create SQL query using natality data after the year 2000
>
> from google.cloud import bigquery
> **query = """**
> **SELECT**
>   **weight_pounds,**
>   **is_male,**
>   **mother_age,**
>   **plurality,**
>   **gestation_weeks,**
>   **ABS(FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING)))) AS hashmonth**
> **FROM**
>   **publicdata.samples.natality**
> **WHERE year > 2000**
> **"""**

There are only a limited number of years and months in the dataset. Let's see what the hashmonths are.

> ##Call BigQuery but GROUP BY the hashmonth and see number of records for each group to enable us to get the correct train and evaluation percentages
>
> **df = bigquery.Client().query("SELECT hashmonth, COUNT(weight_pounds) AS num_babies FROM (" + query + ") GROUP BY hashmonth").to_dataframe()**
> **print("There are {} unique hashmonths.".format(len(df)))**
> **df.head()**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-0.jpg)

Here's a way to get a well distributed portion of the data in such a way that the test and train sets do not overlap:

> ##Added the RAND() so that we can now subsample from each of the hashmonths to get approximately the record counts we want
>
> **trainQuery = "SELECT * FROM (" + query + ") WHERE MOD(hashmonth, 4) < 3 AND RAND() < 0.0005"**
> **evalQuery = "SELECT * FROM (" + query + ") WHERE MOD(hashmonth, 4) = 3 AND RAND() < 0.0005"**
> **traindf = bigquery.Client().query(trainQuery).to_dataframe()**
> **evaldf = bigquery.Client().query(evalQuery).to_dataframe()**
> **print("There are {} examples in the train dataset and {} in the eval dataset".format(len(traindf), len(evaldf)))**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-1.jpg)



<h2>Preprocess data using Pandas

</h2>

Let's add extra rows to simulate the lack of ultrasound. In the process, we'll also change the plurality column to be a string.

> **traindf.head()**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-2.jpg)

Also notice that there are some very important numeric fields that are missing in some rows (the count in Pandas doesn't count missing data)

> ##Let's look at a small sample of the training data
>
> **traindf.describe()**
>
> ##.describe()함수는 DataFrame의 계산 가능한 값들에 대한 다양한 계산 값을 보여준다.

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-3.jpg)

> ##It is always crucial to clean raw data before using in ML, so we have a preprocessing step
>
> **import pandas as pd**
> **def preprocess(df):**
>
> ##clean up data we don't want to train on
> ##in other words, users will have to tell us the mother's age
> ##otherwise, our ML service won't work.
> ##these were chosen because they are such good predictors
> ##and because these are easy enough to collect
>
> **df = df[df.weight_pounds > 0]**
> **df = df[df.mother_age > 0]**
> **df = df[df.gestation_weeks > 0]**
> **df = df[df.plurality > 0]**
>
> ##modify plurality field to be a string 
>
> **twins_etc = dict(zip([1,2,3,4,5],**
>                 **['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)']))**
> **df['plurality'].replace(twins_etc, inplace=True)**
>
> ##zip함수 : 동일한 개수로 이루어진 자료형을 묶어 주는 역할을 하는 함수  (ex. list(zip([1,2,3],[4,5,6])) >> [(1,4),(2,5),(3,6)])
>
> ##now create extra rows to simulate lack of ultrasound**
> **nous = df.copy(deep=True)**
> **nous.loc[nous['plurality'] != 'Single(1)', 'plurality'] = 'Multiple(2+)'**
> **nous['is_male'] = 'Unknown'**
>
> **return pd.concat([df, nous])**

> **traindf.head()**##Let's see a small sample of the training data now after our preprocessing
> **traindf = preprocess(traindf)**
> **evaldf = preprocess(evaldf)**
> **traindf.head()**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-4.jpg)

> **traindf.tail()**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-5.jpg)

> ##Describe only does numeric columns, so you won't see plurality
> **traindf.describe()**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-6.jpg)



<h2>Write out</h2>
In the final versions, we want to read from files, not Pandas dataframes. So, write the Pandas dataframes out as CSV files. Using CSV files gives us the advantage of shuffling during read. This is important for distributed training because some workers might be slower than others, and shuffling the data helps prevent the same data from being assigned to the slow workers. >> Pandas dataframe 이 아니라 csv로 출력.

> **traindf.to_csv('train.csv', index=False, header=False)**
> **evaldf.to_csv('eval.csv', index=False, header=False)**

> **%%bash**
> **wc -l *.csv**
>
> ##wc-l : 파일 내의 라인, 단어 문자의 수를 출력하는 것이 wc. / wc -l : 전체 라인의 수를 출력
>
> **head *.csv**
> **tail *.csv**

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-7.jpg)

![](C:\Users\kyuri\Desktop\github\deeplearning\google export\1-8.jpg)